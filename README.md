# UniDemoir√©: Towards Universal Image Demoir√©ing with Data Generation and Synthesis

<center><a href="https://yizhifengyeyzm.github.io/">Zemin Yang</a><sup>1</sup>, <a href="https://yujingsun.github.io/">Yujing Sun</a><sup>2</sup>, Xidong Peng<sup>1</sup>, <a href="https://www.cs.hku.hk/index.php/people/academic-staff/smyiu/">Siu Ming Yiu</a><sup>2</sup>, <a href="https://yuexinma.me/">Yuexin Ma</a><sup>1</sup></center>

### [Project Page](https://yizhifengyeyzm.github.io/UniDemoire-page/) | [Paper](https://arxiv.org/abs/2502.06324) | Dataset 

***

The generalization ability of SOTA demoir√©ing models is greatly limited by the scarcity of data. Therefore, we mainly face two challenges to obtain a universal model with improved generalization capability: To obtain a vast amount of **1) diverse** and **2) realistic-looking moir√© data**. Notice that traditional moir√© image datasets contain real data, but continuously expanding their size to involve more diversity is extremely time-consuming and impractical. While current synthesized datasets/methods struggle to synthesize realistic-looking moir√© images.

![Pipeline](./static/images/Pipeline.png)

Hence, to tackle these challenges, we introduce a universal solution, **UniDemoir√©**. The data diversity challenge is solved by collecting a more diverse moir√© pattern dataset and presenting a moir√© pattern generator to increase further pattern variations. Meanwhile, the data realistic-looking challenge is undertaken by a moir√© image synthesis module. Finally, our solution can produce realistic-looking moir√© images of sufficient diversity, substantially enhancing the zero-shot and cross-domain performance of demoir√©ing models.

***

## :hourglass_flowing_sand: To Do

- [x] Release training code
- [x] Release testing code
- [ ] Release dataset
- [ ] Release pre-trained models

## üõ†Ô∏è Enviroment
The entire UniDemoir√© framework is built on the Latent Diffusion Model and requires Python 3.8 and PyTorch-Lightning 1.4.2.
You can install the UniDemoir√© environment in the following two ways:
```
conda env create -f environment.yaml
conda activate unidemoire
```
If the installation doesn't go well you can also follow the [instructions](https://github.com/CompVis/latent-diffusion?tab=readme-ov-file#requirements) to install the Latent Diffusion Model environment first, and then install the rest via pip:
```
conda activate unidemoire

...
(install the ldm environment first)
...

pip install colour-demosaicing==0.2.2
pip install thop==0.1.1-2209072238
pip install lpips==0.1.4
pip install timm==0.9.16
pip install pillow==9.5.0
```

## üì¶ Dataset
(to be updated)

## üöÄ Getting Started

>**Some important tips about the training and testing process of our code:**

The style of the config file is similar to [ldm](https://github.com/CompVis/latent-diffusion), and **the paths to the training/testing datasets can be changed inside config.**

Logs and checkpoints for trained models are saved to `logs/<START_DATE_AND_TIME>_<config_spec>`.

**If you need to continue training on a specific model, then you can simply run the training code with the ‚Äú`-r`‚Äù parameter and add your model ckpt path**

The dataset type and path for the test set need to be specified by you in the config file. **The program will automatically start the testing process after training is complete (same pattern as in Latent Diffusion Model)**. If you want to change the test dataset, you need to change the config file, and then re-run your training code with ‚Äú`-r`‚Äù to continue training in the previous step, and the program will go directly to the test session!

If you want to train with multiple gpus, remember to replace `<GPU_ID>` with your gpu id in the code template below, and be sure to adjust the ‚Äú`--gpus`‚Äù parameter that follows it as well 
- For example: if you want to train with `4` gpus (assuming that they are numbered `5`, `6`, `7`, and `8`), then in the code template you should type `CUDA_VISIBLE_DEVICES=5,6,7,8` and with `--gpus 0,1,2,3,`

### Moir√© Pattern Generator

#### 1. AutoEncoder
Configs for training a KL-regularized autoencoder on captured moir√© pattern dataset are provided at `configs/autoencoder`. Training can be started by running:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python main.py --base configs/autoencoder/<config_spec>.yaml --scale_lr False -t --gpus 0,
```
After training, place the ckpt file in `models/moire_generator/autoencoder`.

#### 2. Diffusion Model
In `configs/latent-diffusion/` we provide configs for training diffusion on captured moir√© pattern dataset. Training can be started by running:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python main.py --base configs/latent-diffusion/<config_spec>.yaml -t --gpus 0,
```
After training, place the ckpt file in `models/moire_generator/diffusion`.

#### 3. Sampling
Run the script via:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python scripts/sample_moire_pattern.py 
-r <your_diffusion_model_path.ckpt 
-n <the number of sampled patterns>

For example:
CUDA_VISIBLE_DEVICES=0 python scripts/sample_moire_pattern.py 
-r ./models/moire_generator/diffusion/last.ckpt 
-n 10000
```

### Moir√© Image Synthesis
In `configs/moire-blending/` we provide configs for training the synthesis model on the UHDM, FHDMi, and TIP datasets. Training can be started by running:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python main.py --base configs/moire-blending/<dataset_spec>.yaml --scale_lr False -t --gpus 0,

For example: (training on UHDM dataset)
CUDA_VISIBLE_DEVICES=0 python main.py --base configs/moire-blending/uhdm/blending_uhdm.yaml --scale_lr False -t --gpus 0,
```
where `<dataset_spec>` is one of {`uhdm/blending_uhdm`, `fhdmi/blending_fhdmi`, `tip/blending_tip`}.

After training, place the ckpt file in `models/moire_blending/<dataset>/`. You can find the original config file in these paths. If you want to change the training cofig in the `configs/moire-blending/`, then you also need to change the config file in `models/moire_blending/<dataset>/` accordingly.

### Demoir√©ing

#### 1. Zero-Shot Demoir√©ing
First, download and unzip the MHRNID dataset. **(to be updated)**
Then run the following code to start training on MHRNID:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python main.py --base configs/demoire/mhrnid/<demoire_model_spec>.yaml --scale_lr False -t --gpus 0,

For example: (using ESDNet)
CUDA_VISIBLE_DEVICES=0 python main.py --base configs/demoire/mhrnid/mhrnid_esdnet_unidemoire.yaml --scale_lr False -t --gpus 0,
```
where `<demoire_model_spec>` is one of {`mhrnid_esdnet_unidemoire`, `mhrnid_mbcnn_unidemoire`}.

#### 2. Cross-Dataset Demoir√©ing

Run the following code to start training:
```
CUDA_VISIBLE_DEVICES=<GPU_ID> python main.py --base configs/demoire/cross-dataset/<demoire_model>/<training_dataset>.yaml --scale_lr False -t --gpus 0,

For example: (using ESDNet, train on UHDM dataset)
CUDA_VISIBLE_DEVICES=0 python main.py --base configs/demoire/cross-dataset/esdnet/cd_unidemoire_esdnet_uhdm.yaml --scale_lr False -t --gpus 0,
```
where `<demoire_model>` is one of {`esdnet`, `mbcnn`}, and `<training_dataset>` is one of {`uhdm`, `fhdmi`, `tip`}.



## üôè Acknowledgements

We would like to express our gratitude to the authors and contributors of the following projects:

- [Latent Diffusion Model](https://github.com/CompVis/latent-diffusion)
- [UHDM](https://github.com/CVMI-Lab/UHDM)
- [FHDMi](https://github.com/PKU-IMRE/FHDe2Net)
- [TIP](https://github.com/ZhengJun-AI/MoirePhotoRestoration-MCNN)
- [Uformer](https://github.com/ZhendongWang6/Uformer)
- [UnDeM](https://github.com/zysxmu/UnDeM)



## üìë Citation

If you find our work useful, please consider citing us using the following BibTeX entry:

```
@misc{yang2025unidemoire,
  author    = {Zemin Yang, Yujing Sun, Xidong Peng, Siu Ming Yiu, Yuexin Ma},
  title     = {UniDemoir\'e: Towards Universal Image Demoir\'eing with Data Generation and Synthesis},
  year      = {2025},
  eprint    = {2502.06324},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url={https://arxiv.org/abs/2502.06324},
}
```

